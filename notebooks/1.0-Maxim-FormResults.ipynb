{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import pandas as pd\n",
    "import re\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/maxim/Databases/BRAVE-MASKS_PROCESSED/features/brave_masks_EARLY_features.devel.vad_SileroVoiceActivityDetector.win_4.step_4\n"
     ]
    }
   ],
   "source": [
    "bm_dataset_meta_path = '/media/maxim/Databases/BRAVE-MASKS_PROCESSED/features/brave_masks.devel.vad_SileroVoiceActivityDetector.win_4.step_4'\n",
    "features_type = 'EARLY'\n",
    "basename = os.path.basename(bm_dataset_meta_path)\n",
    "meta_folder = basename.replace('brave_masks.', 'brave_masks_{0}_features.'.format(features_type))\n",
    "meta_folder = bm_dataset_meta_path.replace(os.path.basename(bm_dataset_meta_path), meta_folder)\n",
    "print(meta_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/maxim/Databases/BRAVE-MASKS_PROCESSED/features/brave_masks_EARLY_featuresdevel.vad_SileroVoiceActivityDetector.win_4.step_4'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brave_masks.devel.vad_SileroVoiceActivityDetector.win_4.step_4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename('/media/maxim/Databases/BRAVE-MASKS_PROCESSED/features/brave_masks.devel.vad_SileroVoiceActivityDetector.win_4.step_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/media/maxim/WesternDigital/ABAWLogs/'\n",
    "\n",
    "databases_dir = {'C': 'C'}\n",
    "# databases_dir = {'bravemasks_a': 'BRAVEMASKS/MultimodalAdapted_v2', 'bravemasks_s': 'BRAVEMASKS/MultimodalStandart_v2'}\n",
    "\n",
    "stats_path = 'logs/stats.csv'\n",
    "\n",
    "results = {}\n",
    "for k, db in databases_dir.items():\n",
    "    results[db] = []\n",
    "    exp_lists = os.listdir(os.path.join(root_dir, db))\n",
    "    exp_lists.sort(key=lambda exp_name: os.path.getmtime(os.path.join(root_dir, db, exp_name)), reverse=True)\n",
    "    for exp_name in exp_lists:\n",
    "        df = pd.read_csv(os.path.join(root_dir, db, exp_name, stats_path), sep=';')\n",
    "        best_model = df.iloc[df['abaw_devel_f1'].idxmax()].to_dict()\n",
    "        for name, v in best_model.items():\n",
    "            if 'loss' in name or 'epoch' in name or 'experiment_name' in name:\n",
    "                continue\n",
    "            else:\n",
    "                best_model[name] = v * 100\n",
    "\n",
    "        results[db].append({**{'experiment_name': exp_name}, **best_model})\n",
    "\n",
    "for k, db in results.items():\n",
    "    pd.DataFrame(results[k]).to_csv('{0}_results.csv'.format(k.replace(os.sep, '_')), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.audio_transformers import MyAudioModelPyTorch\n",
    "\n",
    "c_names = ['clear', 'mask']\n",
    "model_cls = MyAudioModelPyTorch\n",
    "models = {\n",
    "    'MyAudioModelPyTorch_WAV2VEC2_BASE': {\n",
    "        'class': model_cls,\n",
    "        'args': {\n",
    "            'inp_size': 16000,\n",
    "            'transformer_params': ['WAV2VEC2_BASE', 768],\n",
    "            'out_size': len(c_names),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "for model_type in models:\n",
    "    wave, sr = torchaudio.load('/media/maxim/Databases/ComParE_2020_PROCESSED/audio/test/test_00001.wav')    \n",
    "    wave = wave.unsqueeze(dim=0)\n",
    "\n",
    "    m1 = models[model_type]['class'](**models[model_type]['args'])\n",
    "    m2 = models[model_type]['class'](**models[model_type]['args'])\n",
    "    m2.load_state_dict(torch.load('../models/epoch_111.pth')['model_state_dict'])\n",
    "    \n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "    with torch.no_grad():\n",
    "#         print(m1.stp.register_forward_hook(get_features('stp')))\n",
    "#         print(m2.stp.register_forward_hook(get_features('stp')))\n",
    "        print(wave)\n",
    "        print(wave.shape)\n",
    "        z1 = m1(wave)\n",
    "        print(wave)\n",
    "        print(wave.shape)\n",
    "        print(m1.extract_fe(wave))\n",
    "        \n",
    "        z2 = m2(wave)\n",
    "        print(m2.extract_fe(wave))\n",
    "        \n",
    "        \n",
    "#         print(z1.)\n",
    "    \n",
    "#     m1.fc1 = torch.nn.Identity()\n",
    "#     m2.fc1 = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 16000])\n",
      "tensor([[ 5.5454e-02,  1.9580e-01, -9.2937e-04,  1.2593e-01,  5.5537e-01,\n",
      "         -1.1019e-03,  7.6513e-02,  8.6004e-02,  2.9210e-02,  3.4305e-01,\n",
      "         -2.6372e-04, -1.1736e-03,  3.8961e-02,  5.4420e-01, -2.5212e-03,\n",
      "         -2.6131e-03, -1.6518e-04, -3.0438e-03, -6.7908e-04,  7.7389e-02,\n",
      "         -2.2126e-03,  3.4881e-02, -3.1008e-03, -1.4076e-03,  8.3789e-03,\n",
      "          5.6141e-01,  2.2627e-01,  2.0179e-01,  5.7657e-02, -1.8160e-04,\n",
      "          1.1726e-01, -1.8823e-03, -2.9266e-03, -8.9208e-04,  2.8181e-01,\n",
      "          6.0142e-01, -1.2599e-03,  4.1089e-01, -5.6477e-04, -2.0832e-03,\n",
      "          2.2378e-01,  2.2142e-02, -8.0184e-04, -3.1103e-03, -1.4273e-03,\n",
      "         -1.1196e-03,  1.2150e-02,  4.4499e-01,  7.3128e-02,  3.3560e-01,\n",
      "          1.9592e-01, -2.9776e-03,  4.4328e-02, -1.2541e-03, -9.4062e-04,\n",
      "          3.4709e-01, -1.6911e-03, -4.5928e-04,  4.1844e-01, -3.3037e-03,\n",
      "          5.7833e-02, -4.9643e-04, -5.9883e-04, -2.8486e-05,  6.8279e-01,\n",
      "          4.1655e-02,  3.2459e-02, -1.7164e-03, -7.1208e-04, -1.9524e-04,\n",
      "         -5.5158e-04,  3.7742e-01,  2.4904e-01,  3.0388e-01, -9.5508e-04,\n",
      "          4.3843e-01, -2.0106e-04,  8.5721e-02, -2.5281e-03, -1.4323e-03,\n",
      "          1.2771e-01,  3.8886e-03,  5.7807e-01, -2.0189e-03,  1.9089e-01,\n",
      "         -1.6759e-03,  3.6308e-01, -1.2409e-03, -1.9118e-03,  9.0061e-02,\n",
      "          3.7239e-01, -2.4784e-03, -4.0858e-04,  5.7887e-01,  4.4863e-01,\n",
      "          6.2579e-02,  4.5270e-01, -1.6518e-03, -7.1225e-04,  5.2863e-01,\n",
      "         -1.9978e-03, -1.8188e-03, -5.6791e-04, -2.3603e-03,  4.8142e-01,\n",
      "         -8.0136e-04, -2.2356e-03, -3.7662e-04, -8.4817e-04, -1.2096e-03,\n",
      "          4.6257e-02, -1.5335e-03, -1.8241e-03,  1.4509e-01,  7.3404e-02,\n",
      "          2.4446e-01, -1.2166e-03,  5.3165e-01, -1.1336e-03, -3.0921e-03,\n",
      "          4.1522e-01, -2.7606e-03, -2.1230e-03,  1.8715e-01, -7.3758e-04,\n",
      "         -1.3255e-03,  6.6981e-02,  1.4563e-01,  3.4741e-01,  3.1129e-01,\n",
      "         -2.6686e-03,  1.4181e-01, -2.4542e-03,  2.8395e-02,  4.3307e-01,\n",
      "         -1.8803e-03,  7.9623e-01, -2.6229e-04,  4.9428e-01,  3.0677e-01,\n",
      "          4.0965e-01, -1.1796e-04,  4.7478e-01, -1.5832e-03, -2.1864e-03,\n",
      "          7.2391e-01,  1.3905e-02,  1.1253e-01, -1.2912e-03,  2.6291e-02,\n",
      "         -4.8971e-04,  3.2075e-01, -1.9767e-03,  7.9684e-02,  4.2558e-02,\n",
      "         -1.4726e-03, -1.7431e-03, -1.1154e-03,  5.8906e-02,  3.1294e-01,\n",
      "         -1.5933e-03, -3.5565e-04,  6.0219e-01, -1.0298e-03, -1.1459e-03,\n",
      "         -2.3845e-03, -3.1149e-04, -1.4968e-03,  3.7011e-02, -3.6348e-04,\n",
      "         -3.8670e-03, -6.7181e-04, -2.1336e-03,  2.2285e-01,  1.7868e-01,\n",
      "         -3.3772e-04,  5.2233e-01, -1.1267e-03, -1.2213e-03, -2.7495e-03,\n",
      "          2.0354e-01,  2.1032e-03, -5.1657e-03,  1.2685e-01, -2.8046e-03,\n",
      "          2.2394e-01,  4.7820e-01,  1.1394e-02,  1.5018e-02,  6.1664e-01,\n",
      "          4.4422e-02, -1.3822e-03, -8.3220e-04,  1.1973e-03,  4.5119e-02,\n",
      "         -1.0765e-03, -2.7341e-03, -2.8843e-03, -7.7557e-04, -3.7952e-03,\n",
      "         -7.6894e-04, -5.3369e-04,  2.9584e-01, -1.8876e-03, -3.2252e-03,\n",
      "         -9.9433e-04,  1.9271e-01,  3.0825e-01, -1.7094e-03, -1.5035e-03,\n",
      "         -5.7938e-04,  4.8870e-01, -3.6373e-03,  1.5619e-01, -1.1358e-03,\n",
      "         -5.8981e-04, -2.1874e-03,  4.0689e-01, -1.5585e-03,  9.1699e-03,\n",
      "         -3.6984e-04,  4.0290e-01,  4.0173e-01, -1.1545e-03,  1.0754e-01,\n",
      "          4.7162e-02, -9.3375e-04, -1.9408e-03,  3.2018e-01,  3.8784e-01,\n",
      "          5.7343e-01, -2.2619e-03, -3.7668e-04,  2.9351e-02,  1.2817e-01,\n",
      "         -1.1649e-03, -2.9847e-04, -8.8221e-04,  7.5623e-02, -2.6644e-04,\n",
      "          2.4551e-01, -3.5849e-03, -3.7881e-03, -1.2439e-03,  1.7387e-01,\n",
      "         -7.7048e-04, -6.0262e-04, -2.3311e-03, -1.8797e-04, -3.8289e-03,\n",
      "         -2.2963e-04,  3.4646e-01, -5.0089e-04, -2.0519e-03,  1.6369e-01,\n",
      "         -1.2470e-03, -3.7317e-03,  1.3610e-01, -8.6147e-04,  1.3667e-01,\n",
      "          2.6981e-02,  4.7027e-01,  6.1405e-01,  5.4070e-01,  2.8480e-01,\n",
      "          1.1219e-01,  5.6048e-01,  2.6170e-01,  6.4959e-02,  2.9220e-01,\n",
      "          2.3090e-01, -3.8811e-04,  6.2509e-01, -7.3603e-04,  5.8305e-01,\n",
      "         -1.9689e-04,  3.3687e-02,  9.5870e-02,  5.5785e-01, -3.3377e-04,\n",
      "          4.6183e-01,  2.0008e-01, -2.1027e-03, -1.3364e-03,  4.1295e-01,\n",
      "          2.7918e-02, -1.0453e-03,  2.2689e-01,  6.7525e-02, -9.9600e-04,\n",
      "          2.1055e-01, -1.1363e-03, -1.1679e-03,  3.2094e-03, -1.3082e-03,\n",
      "          1.6622e-01, -4.3388e-04,  6.2878e-02, -1.8173e-03, -2.6283e-03,\n",
      "         -7.0971e-04,  2.8253e-01, -4.0369e-04, -1.3804e-03, -3.2620e-03,\n",
      "         -2.3075e-04,  4.9442e-01,  3.0790e-01, -3.4223e-03, -6.2903e-04,\n",
      "          2.8637e-02,  1.9136e-01, -1.8373e-03,  2.9850e-01, -8.0880e-04,\n",
      "         -1.1314e-03,  3.4483e-01, -9.3045e-04, -1.9785e-03, -1.5351e-03,\n",
      "          2.0333e-02, -2.2225e-03,  2.9540e-01,  1.3587e-01, -1.4141e-03,\n",
      "         -1.0900e-03, -3.0373e-03,  1.1426e-01,  8.7152e-02,  2.9611e-02,\n",
      "          8.8143e-02, -1.9736e-03,  5.6457e-01, -5.5490e-04, -7.9212e-04,\n",
      "         -9.2854e-04, -9.8415e-04, -2.3841e-03,  4.9895e-01,  1.2097e-01,\n",
      "         -1.1746e-03,  5.4726e-01, -8.7529e-04,  6.3959e-01, -1.4408e-03,\n",
      "         -8.8634e-04, -1.2374e-03,  5.9982e-01, -7.5553e-04,  6.0723e-01,\n",
      "         -4.9419e-04, -8.9947e-04, -2.5093e-03, -1.3476e-03, -2.7608e-03,\n",
      "          3.6071e-02,  7.7009e-02, -2.8322e-04, -1.5756e-03,  4.4867e-02,\n",
      "          4.5843e-01,  4.2806e-01, -5.9257e-04, -2.3048e-03, -2.9477e-03,\n",
      "         -2.1444e-03,  3.5707e-01,  3.4672e-01,  4.4770e-01,  1.3967e-01,\n",
      "         -7.9425e-04,  7.6020e-02,  1.2043e-02,  1.0373e-01, -1.4386e-03,\n",
      "          2.4134e-02, -1.5771e-03,  3.2473e-01,  6.2068e-01,  6.4812e-02,\n",
      "         -2.3876e-03,  1.2770e-02, -1.4807e-03,  3.1716e-02, -8.0463e-04,\n",
      "         -6.5447e-04, -2.0867e-03,  3.6399e-01, -3.3900e-04,  3.0613e-02,\n",
      "          4.1859e-01, -2.1150e-05, -1.3551e-03, -4.2272e-04, -1.0131e-03,\n",
      "         -1.3568e-03, -1.5649e-03,  8.3210e-01, -2.1008e-03,  3.6608e-01,\n",
      "         -9.2082e-05, -6.8561e-04, -1.1200e-03, -1.9628e-03,  2.2171e-01,\n",
      "         -1.7657e-03, -1.0108e-04,  2.3722e-01, -3.6944e-03,  4.9700e-01,\n",
      "         -1.5280e-03, -1.1064e-03,  3.7745e-01, -2.1521e-03, -4.4769e-04,\n",
      "          2.0389e-02, -6.3602e-04,  4.1427e-01, -1.3628e-03,  6.8655e-02,\n",
      "          4.0491e-01, -6.3854e-04, -3.4790e-04,  2.7362e-02,  4.2872e-01,\n",
      "         -1.2463e-03, -2.3450e-04,  6.2556e-02, -7.5240e-04, -3.3647e-03,\n",
      "         -1.6289e-03, -2.0276e-03, -1.3624e-03,  5.1135e-01, -9.2136e-04,\n",
      "          1.6008e-01, -1.6520e-03,  2.9736e-01, -9.6693e-04,  6.4345e-02,\n",
      "          3.9108e-02,  6.4036e-01, -1.3775e-03, -1.2675e-03, -9.0592e-04,\n",
      "         -2.6684e-03, -2.5359e-04,  4.5376e-01, -1.4695e-04, -1.6906e-03,\n",
      "         -5.0433e-04, -1.5913e-03, -3.2713e-03, -3.2537e-03, -2.3754e-03,\n",
      "          3.2407e-01,  4.9108e-01, -2.1313e-04, -1.2156e-03,  3.4402e-02,\n",
      "          3.1267e-01,  1.1618e-01, -1.6342e-03,  2.6790e-02, -4.6638e-04,\n",
      "          4.1948e-03, -7.6381e-05,  3.7954e-01, -2.5390e-03,  1.1665e-01,\n",
      "         -5.1794e-04, -1.5218e-03, -1.5178e-03, -7.9027e-04,  9.1413e-02,\n",
      "          1.0978e-01, -2.2918e-04, -2.3301e-03, -1.1111e-03, -1.5806e-03,\n",
      "         -2.8687e-03, -1.9322e-04, -4.0388e-04,  3.5149e-03,  1.0006e-01,\n",
      "         -1.5766e-03, -1.2590e-03, -1.1078e-03, -3.6977e-03,  2.9969e-01,\n",
      "          3.6288e-01,  6.8422e-01, -7.1317e-04,  5.1573e-01,  3.7406e-01,\n",
      "          2.7744e-01, -3.2331e-03, -1.6232e-03,  8.6674e-02, -1.2105e-03,\n",
      "         -3.7132e-04, -2.1842e-03, -2.5354e-03, -2.8804e-04,  2.1642e-01,\n",
      "         -1.6073e-03, -9.7937e-04,  2.6821e-02, -3.0406e-03, -7.2268e-04,\n",
      "         -8.2774e-04,  3.0284e-02]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.audio_transformers import MyAudioModelPyTorch\n",
    "\n",
    "c_names = ['clear', 'mask']\n",
    "model_cls = MyAudioModelPyTorch\n",
    "models = {\n",
    "    'MyAudioModelPyTorch_WAV2VEC2_BASE': {\n",
    "        'class': model_cls,\n",
    "        'args': {\n",
    "            'inp_size': 16000,\n",
    "            'transformer_params': ['WAV2VEC2_BASE', 768],\n",
    "            'out_size': len(c_names),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "    return hook\n",
    "    \n",
    "for model_type in models:\n",
    "    wave, sr = torchaudio.load('/media/maxim/Databases/ComParE_2020_PROCESSED/audio/test/test_00001.wav')    \n",
    "    wave = wave.unsqueeze(dim=0)\n",
    "    print(wave.shape)\n",
    "    m1 = models[model_type]['class'](**models[model_type]['args'])\n",
    "    m2 = models[model_type]['class'](**models[model_type]['args'])\n",
    "    m2.load_state_dict(torch.load('../models/epoch_111.pth')['model_state_dict'])\n",
    "    \n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "    with torch.no_grad():\n",
    "#         print(m1.stp.register_forward_hook(get_features('stp')))\n",
    "#         print(m2.stp.register_forward_hook(get_features('stp')))\n",
    "        \n",
    "#         z1 = m1(wave)\n",
    "        z2 = m2(wave)\n",
    "        \n",
    "#         print(z1.)\n",
    "    \n",
    "#     m1.fc1 = torch.nn.Identity()\n",
    "#     m2.fc1 = torch.nn.Identity()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 15:04:40.160942: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_x = np.random.rand(1, 224, 224, 3).astype('float32')\n",
    "pth_x = torch.Tensor(np.transpose(tf_x, (0, 3, 1, 2)))\n",
    "\n",
    "tf_l = tf.keras.models.Model(inputs=[tf_model.get_layer('input_1').input], \n",
    "                             outputs=[tf_model.get_layer('dense').output])\n",
    "\n",
    "tf_out = tf_l(tf_x, training=False).numpy()\n",
    "\n",
    "pth_model.eval()\n",
    "with torch.no_grad():\n",
    "    pth_out = F.softmax(pth_model(pth_x), dim=1).detach().numpy()\n",
    "np.allclose(tf_out, pth_out, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch': 0,\n",
    "    'model_state_dict': pth_model.state_dict(),\n",
    "    'optimizer_state_dict': None,\n",
    "    'loss': None\n",
    "}, 'pth_image_mask_detection.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "m = LenaResNet()\n",
    "m.load_state_dict(torch.load('pth_image_mask_detection.h5')['model_state_dict'])\n",
    "m.eval()\n",
    "with torch.no_grad():\n",
    "    m_out = F.softmax(m(pth_x), dim=1).detach().numpy()\n",
    "    \n",
    "print(np.allclose(tf_out, m_out, atol=1e-5))\n",
    "print(np.allclose(m_out, pth_out, atol=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 690ms/step\n",
      "[[0.03960168 0.9603984 ]]\n",
      "Predicted class:  With_Mask\n",
      "Confidence: 96.04%\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocess_input(x, data_format=None, version=1):\n",
    "    x_temp = np.copy(x)\n",
    "    if data_format is None:\n",
    "        data_format = K.image_data_format()\n",
    "    assert data_format in {'channels_last', 'channels_first'}\n",
    "\n",
    "    if data_format == 'channels_first':\n",
    "        x_temp = x_temp[:, ::-1, ...]\n",
    "        x_temp[:, 0, :, :] -= 91.4953\n",
    "        x_temp[:, 1, :, :] -= 103.8827\n",
    "        x_temp[:, 2, :, :] -= 131.0912\n",
    "    else:\n",
    "        x_temp = x_temp[..., ::-1]\n",
    "        x_temp[..., 0] -= 91.4953\n",
    "        x_temp[..., 1] -= 103.8827\n",
    "        x_temp[..., 2] -= 131.0912\n",
    "\n",
    "    return x_temp\n",
    "\n",
    "t_img = cv2.imread('0_0_0.jpg')\n",
    "t_img = cv2.cvtColor(t_img, cv2.COLOR_BGR2RGB)\n",
    "# x = tf.cast(x, np.uint8)\n",
    "t_img = tf.image.resize(t_img, (224,224))\n",
    "t_img = tf.keras.preprocessing.image.img_to_array(t_img)\n",
    "t_img = preprocess_input(t_img)\n",
    "t_img = np.expand_dims(t_img, axis=0)\n",
    "dict = {0: 'Without_Mask', 1: 'With_Mask'}\n",
    "prob = tf_model.predict(t_img)\n",
    "print(prob)\n",
    "cl = np.argmax(prob)\n",
    "print('Predicted class: ', dict[cl])\n",
    "print('Confidence: {}%'.format(np.round(prob[0][cl]*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03922091 0.9607791 ]]\n",
      "Predicted class:  With_Mask\n",
      "Confidence: 96.08%\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class PreprocessInput(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PreprocessInput, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = np.asarray(x, dtype='float64')\n",
    "        x = x[..., ::-1]\n",
    "        x[..., 0] -= 91.4953\n",
    "        x[..., 1] -= 103.8827\n",
    "        x[..., 2] -= 131.0912\n",
    "        x = torch.Tensor(x.copy())\n",
    "        return x.permute(2, 0, 1)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    PreprocessInput(),\n",
    "])\n",
    "\n",
    "p_img = Image.open('0_0_0.jpg')\n",
    "p_img = preprocess(p_img)\n",
    "p_img = torch.unsqueeze(p_img, 0)\n",
    "\n",
    "pth_model.eval()\n",
    "with torch.no_grad():\n",
    "    res = F.softmax(pth_model(p_img), dim=1).detach().numpy()\n",
    "\n",
    "print(res)\n",
    "cl = np.argmax(res)\n",
    "print('Predicted class: ', dict[cl])\n",
    "print('Confidence: {}%'.format(np.round(res[0][cl]*100, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
